{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time': '2024-08-05T03:03:42.109643248Z',\n",
       " 'prices': [{'type': 'PRICE',\n",
       "   'time': '2024-08-05T03:03:41.035879122Z',\n",
       "   'bids': [{'price': '1.09230', 'liquidity': 500000},\n",
       "    {'price': '1.09229', 'liquidity': 2500000},\n",
       "    {'price': '1.09228', 'liquidity': 2000000},\n",
       "    {'price': '1.09227', 'liquidity': 5000000},\n",
       "    {'price': '1.09224', 'liquidity': 10000000},\n",
       "    {'price': '1.09221', 'liquidity': 10000000}],\n",
       "   'asks': [{'price': '1.09246', 'liquidity': 500000},\n",
       "    {'price': '1.09247', 'liquidity': 500000},\n",
       "    {'price': '1.09248', 'liquidity': 2000000},\n",
       "    {'price': '1.09249', 'liquidity': 2000000},\n",
       "    {'price': '1.09250', 'liquidity': 5000000},\n",
       "    {'price': '1.09252', 'liquidity': 10000000},\n",
       "    {'price': '1.09255', 'liquidity': 10000000}],\n",
       "   'closeoutBid': '1.09221',\n",
       "   'closeoutAsk': '1.09255',\n",
       "   'status': 'tradeable',\n",
       "   'tradeable': True,\n",
       "   'quoteHomeConversionFactors': {'positiveUnits': '1.00000000',\n",
       "    'negativeUnits': '1.00000000'},\n",
       "   'instrument': 'EUR_USD'}]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "ACCOUNT_ID = \"001-001-2265866-001\"\n",
    "API_TOKEN = \"591691d91a571232b46b9b87e74145ba-4fc5bc3427b4eb7d79c3501316fdffa0\"\n",
    "\n",
    "API_URL = \"api-fxtrade.oanda.com\"\n",
    "STREAM_URL = \"stream-fxtrade.oanda.com\"\n",
    "\n",
    "PRICING_PATH = f\"/v3/accounts/{ACCOUNT_ID}/pricing\"\n",
    "\n",
    "query = {\"instruments\": \"EUR_USD\"} \n",
    "headers = {\"Authorization\": \"Bearer \"+ API_TOKEN}\n",
    "\n",
    "response = requests.get(\"https://\"+API_URL+PRICING_PATH, headers=headers, params=query) \n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'candles'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m oa \u001b[38;5;241m=\u001b[39m OandaAPI(account_id\u001b[38;5;241m=\u001b[39mACCOUNT_ID, api_token\u001b[38;5;241m=\u001b[39mAPI_TOKEN)\n\u001b[1;32m      4\u001b[0m gbp_jpy_df \u001b[38;5;241m=\u001b[39m oa\u001b[38;5;241m.\u001b[39mshow_granularity()\n\u001b[0;32m----> 5\u001b[0m shit_df \u001b[38;5;241m=\u001b[39m \u001b[43moa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistorical_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m gbp_jpy_df\n",
      "File \u001b[0;32m~/Development/Algo-Research/OandaAPI.py:89\u001b[0m, in \u001b[0;36mOandaAPI.historical_data\u001b[0;34m(self, currency_pair, start_date, end_date, granularity, as_pandas)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Create Pandas DataFrame\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_pandas \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# Convert Json to Pandas df\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     candle_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mjson_normalize(\u001b[43mjson_response\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcandles\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Drop complete Column\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     candle_df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'candles'"
     ]
    }
   ],
   "source": [
    "from OandaAPI import OandaAPI\n",
    "oa = OandaAPI(account_id=ACCOUNT_ID, api_token=API_TOKEN)\n",
    "\n",
    "gbp_jpy_df = oa.show_granularity()\n",
    "shit_df = oa.historical_data()\n",
    "\n",
    "gbp_jpy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-08-01 05:00:00+00:00</th>\n",
       "      <td>1.09904</td>\n",
       "      <td>1.10004</td>\n",
       "      <td>1.09868</td>\n",
       "      <td>1.09948</td>\n",
       "      <td>7560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-01 07:00:00+00:00</th>\n",
       "      <td>1.09944</td>\n",
       "      <td>1.09949</td>\n",
       "      <td>1.09702</td>\n",
       "      <td>1.09748</td>\n",
       "      <td>11225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-01 09:00:00+00:00</th>\n",
       "      <td>1.09749</td>\n",
       "      <td>1.09847</td>\n",
       "      <td>1.09658</td>\n",
       "      <td>1.09822</td>\n",
       "      <td>8147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-01 11:00:00+00:00</th>\n",
       "      <td>1.09822</td>\n",
       "      <td>1.09824</td>\n",
       "      <td>1.09524</td>\n",
       "      <td>1.09565</td>\n",
       "      <td>10231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-01 13:00:00+00:00</th>\n",
       "      <td>1.09566</td>\n",
       "      <td>1.09892</td>\n",
       "      <td>1.09560</td>\n",
       "      <td>1.09783</td>\n",
       "      <td>16453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-31 19:00:00+00:00</th>\n",
       "      <td>1.08265</td>\n",
       "      <td>1.08329</td>\n",
       "      <td>1.08170</td>\n",
       "      <td>1.08264</td>\n",
       "      <td>10399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-31 21:00:00+00:00</th>\n",
       "      <td>1.08248</td>\n",
       "      <td>1.08310</td>\n",
       "      <td>1.08241</td>\n",
       "      <td>1.08243</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-31 23:00:00+00:00</th>\n",
       "      <td>1.08246</td>\n",
       "      <td>1.08301</td>\n",
       "      <td>1.08218</td>\n",
       "      <td>1.08277</td>\n",
       "      <td>7084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-01 01:00:00+00:00</th>\n",
       "      <td>1.08276</td>\n",
       "      <td>1.08338</td>\n",
       "      <td>1.08243</td>\n",
       "      <td>1.08262</td>\n",
       "      <td>8848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-01 03:00:00+00:00</th>\n",
       "      <td>1.08261</td>\n",
       "      <td>1.08354</td>\n",
       "      <td>1.08256</td>\n",
       "      <td>1.08268</td>\n",
       "      <td>6637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3120 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Open     High      Low    Close  Volume\n",
       "DateTime                                                             \n",
       "2023-08-01 05:00:00+00:00  1.09904  1.10004  1.09868  1.09948    7560\n",
       "2023-08-01 07:00:00+00:00  1.09944  1.09949  1.09702  1.09748   11225\n",
       "2023-08-01 09:00:00+00:00  1.09749  1.09847  1.09658  1.09822    8147\n",
       "2023-08-01 11:00:00+00:00  1.09822  1.09824  1.09524  1.09565   10231\n",
       "2023-08-01 13:00:00+00:00  1.09566  1.09892  1.09560  1.09783   16453\n",
       "...                            ...      ...      ...      ...     ...\n",
       "2024-07-31 19:00:00+00:00  1.08265  1.08329  1.08170  1.08264   10399\n",
       "2024-07-31 21:00:00+00:00  1.08248  1.08310  1.08241  1.08243     906\n",
       "2024-07-31 23:00:00+00:00  1.08246  1.08301  1.08218  1.08277    7084\n",
       "2024-08-01 01:00:00+00:00  1.08276  1.08338  1.08243  1.08262    8848\n",
       "2024-08-01 03:00:00+00:00  1.08261  1.08354  1.08256  1.08268    6637\n",
       "\n",
       "[3120 rows x 5 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACCOUNT_ID = \"001-001-2265866-001\"\n",
    "API_TOKEN = \"591691d91a571232b46b9b87e74145ba-4fc5bc3427b4eb7d79c3501316fdffa0\"\n",
    "\n",
    "API_URL = \"api-fxtrade.oanda.com\"\n",
    "STREAM_URL = \"stream-fxtrade.oanda.com\"\n",
    "\n",
    "PRICING_PATH = f\"/v3/accounts/{ACCOUNT_ID}/pricing\"\n",
    "\n",
    "def get_historical_data(currency_pair='EUR_USD', start_date='08/09/2023', end_date='08/09/2024', granularity='D' ,as_pandas=True):\n",
    "    \"\"\"Will get the historical json data for a given currency pair.\n",
    "\n",
    "    Args:\n",
    "        currency_pair: Currency pair name. \n",
    "        start_date: Date of start of historical data.\n",
    "        end_date: Closest date of historical data. \n",
    "        granularity: How often price data is updated.\n",
    "        as_pandas: Set to True to return as pandas dataframe.\n",
    "\n",
    "    Returns:\n",
    "        json: Historical pricing data in json format.\n",
    "        Pandas: Historical pricing dataframe.\n",
    "    \"\"\"\n",
    "    # Must import Necessary Libraries\n",
    "    import time\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "\n",
    "    # Convert Dates to DateTime objects\n",
    "    start_dateTime = pd.to_datetime(start_date)\n",
    "    end_dateTime = pd.to_datetime(end_date)\n",
    "\n",
    "    # Convert Start Date & End Date to a UNIX timestamp\n",
    "    unix_start = time.mktime(start_dateTime.timetuple())\n",
    "    unix_end = time.mktime(end_dateTime.timetuple())\n",
    "\n",
    "    # Authorization Header\n",
    "    auth_header = {\"Authorization\": \"Bearer \"+API_TOKEN} \n",
    "\n",
    "    # Query Parameters\n",
    "    parameters = {\"from\": str(unix_start), \n",
    "                    \"to\": str(unix_end), \n",
    "                    \"granularity\": granularity}\n",
    "    \n",
    "    # Historical Candle Stick Data URL\n",
    "    candle_url = f\"/v3/accounts/{ACCOUNT_ID}/instruments/{currency_pair}/candles\"\n",
    "\n",
    "    # Request for Candle Stick Data\n",
    "    json_response = requests.get(\"https://\"+API_URL+candle_url, headers=auth_header, params=parameters).json()\n",
    "\n",
    "    # Create Pandas DataFrame\n",
    "    if as_pandas is True:\n",
    "        # Convert Json to Pandas df\n",
    "        candle_df = pd.json_normalize(json_response[\"candles\"])\n",
    "\n",
    "        # Drop complete Column\n",
    "        candle_df.drop('complete', inplace=True, axis=1)\n",
    "\n",
    "        # Rename Columns\n",
    "        candle_df.rename(inplace=True,columns={'time':'DateTime','volume':'Volume','mid.o':'Open',\n",
    "                                                'mid.h':'High','mid.l':'Low','mid.c':'Close'})\n",
    "\n",
    "        # Convert Datetime= Column to datetime object\n",
    "        candle_df['DateTime'] = pd.to_datetime(candle_df['DateTime'])\n",
    "\n",
    "        # Set DateTime to Index\n",
    "        candle_df.set_index('DateTime',inplace=True)\n",
    "\n",
    "        # Restructure Column Names\n",
    "        candle_df = candle_df[['Open','High','Low','Close','Volume']]\n",
    "        \n",
    "        # Return DataFrame\n",
    "        return candle_df\n",
    "    \n",
    "    # Return as Json \n",
    "    else: \n",
    "        return json_response\n",
    "\n",
    "df = get_historical_data(currency_pair='EUR_USD', \n",
    "                     start_date='08/01/2023',\n",
    "                    end_date='08/01/2024',\n",
    "                    granularity='H2',\n",
    "                    as_pandas=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time    \n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "def get_all_candles(start,end,gran):\n",
    "\n",
    "    gran_secs = {'S5': 5,'S10': 10,'S15': 15,'S30': 30,\n",
    "                 'M1': 60,'M2': 120,'M4': 240,'M5': 300,\n",
    "                 'M10': 600,'M15': 900,'M30': 1800,\n",
    "                 'H1': 3600,'H2': 7200,'H3': 10800,\n",
    "                 'H4': 14400,'H6': 21600,'H8': 28800,\n",
    "                 'H12': 43200,'D': 86400,'W': 604800,\n",
    "                 'M': 2620800}\n",
    "\n",
    "    print(f'start Date:{start} & end Date:{end}')\n",
    "    start_dateTime = pd.to_datetime(start)\n",
    "    end_dateTime = pd.to_datetime(end)\n",
    "    print(f'start DateTime:{start_dateTime} & end DateTime:{end_dateTime}')\n",
    "\n",
    "\n",
    "    time_distance = end_dateTime - start_dateTime   \n",
    "    seconds_between = time_distance.total_seconds() \n",
    "    num_data_points = seconds_between / gran_secs.get(gran)\n",
    "    print(f'Seconds Between:{seconds_between}')\n",
    "\n",
    "\n",
    "    if num_data_points > 4000:\n",
    "        print('Greater than 4000 seconds')\n",
    "        max_date = start_dateTime+timedelta(seconds=4000*math.floor(gran_secs.get(gran)))\n",
    "        print(f'First Max date: {max_date}')\n",
    "\n",
    "        # Create Pandas DataFrame\n",
    "        can_df = pd.DataFrame()\n",
    "\n",
    "        new_start_dateTime = start_dateTime\n",
    "       \n",
    "        while True:\n",
    "            if max_date >= end_dateTime:\n",
    "                print(f'{new_start_dateTime} - {end_dateTime}')\n",
    "                new_df = test_get_historical_data(instrument='EUR_USD', \n",
    "                                            start_date=new_start_dateTime,\n",
    "                                            end_date=end_dateTime,\n",
    "                                            granularity=gran)\n",
    "                can_df = pd.concat([can_df, new_df])\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                print(f'{new_start_dateTime} - {max_date}')\n",
    "                new_df = test_get_historical_data(instrument='EUR_USD', \n",
    "                                                start_date=new_start_dateTime,\n",
    "                                                end_date=max_date,\n",
    "                                                granularity=gran)\n",
    "                new_start_dateTime = max_date+timedelta(seconds=gran_secs.get(gran))\n",
    "                max_date = new_start_dateTime+timedelta(seconds=4000*math.floor(gran_secs.get(gran)))\n",
    "                can_df = pd.concat([can_df, new_df])\n",
    "                                        \n",
    "        return can_df\n",
    "    else: \n",
    "        print(f'{start_dateTime} - {end_dateTime}')\n",
    "        can_df = test_get_historical_data(instrument='EUR_USD', \n",
    "                                    start_date=start_dateTime,\n",
    "                                    end_date=end_dateTime,\n",
    "                                    granularity=gran)\n",
    "\n",
    "        return can_df\n",
    "\n",
    "master_df = get_all_candles(start='08/01/2005',\n",
    "                            end='08/01/2024',\n",
    "                            gran='M5')\n",
    "\n",
    "master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_all_candles_upgraded() got an unexpected keyword argument 'pair'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 72\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m         new_df \u001b[38;5;241m=\u001b[39m historical_data(currency_pair\u001b[38;5;241m=\u001b[39mcurrency_pair, \n\u001b[1;32m     67\u001b[0m                                           start_date\u001b[38;5;241m=\u001b[39mstart_dateTime_list[i],\n\u001b[1;32m     68\u001b[0m                                           end_date\u001b[38;5;241m=\u001b[39mend_dateTime_list[i],\n\u001b[1;32m     69\u001b[0m                                           granularity\u001b[38;5;241m=\u001b[39mgran)\n\u001b[0;32m---> 72\u001b[0m master_df \u001b[38;5;241m=\u001b[39m \u001b[43mget_all_candles_upgraded\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m08/01/2024\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m08/02/2024\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mgran\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mM1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mpair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEUR_USD\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m master_df\n",
      "\u001b[0;31mTypeError\u001b[0m: get_all_candles_upgraded() got an unexpected keyword argument 'pair'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "import tqdm\n",
    "def get_all_candles_upgraded(start,end,gran, verbose=False, currency_pair='EUR_USD'):\n",
    "\n",
    "    gran_secs = {'S5': 5,'S10': 10,'S15': 15,'S30': 30,\n",
    "                 'M1': 60,'M2': 120,'M4': 240,'M5': 300,\n",
    "                 'M10': 600,'M15': 900,'M30': 1800,\n",
    "                 'H1': 3600,'H2': 7200,'H3': 10800,\n",
    "                 'H4': 14400,'H6': 21600,'H8': 28800,\n",
    "                 'H12': 43200,'D': 86400,'W': 604800,\n",
    "                 'M': 2620800}\n",
    "\n",
    "    MAX_CANDLES = 4000\n",
    "\n",
    "    start_dateTime = pd.to_datetime(start)\n",
    "    end_dateTime = pd.to_datetime(end)\n",
    "    print(f'Gathering Data {start_dateTime} - {end_dateTime}')\n",
    "\n",
    "\n",
    "    \n",
    "    time_distance = end_dateTime - start_dateTime   \n",
    "    seconds_between = time_distance.total_seconds() \n",
    "    num_candles = seconds_between / gran_secs.get(gran)\n",
    "    print(f'Seconds Between:{seconds_between}')\n",
    "\n",
    "    if num_candles > MAX_CANDLES:\n",
    "        max_date = start_dateTime+timedelta(seconds=MAX_CANDLES*math.floor(gran_secs.get(gran)))\n",
    "\n",
    "        # Create Pandas DataFrame\n",
    "        can_df = pd.DataFrame()\n",
    "        start_dateTime_list = []\n",
    "        end_dateTime_list = []\n",
    "\n",
    "        new_start_dateTime = start_dateTime\n",
    "\n",
    "        while True:\n",
    "            if max_date >= end_dateTime:\n",
    "                start_dateTime_list.append(new_start_dateTime)\n",
    "                end_dateTime_list.append(end_dateTime)\n",
    "                print(f'Last Line: {new_start_dateTime} - {end_dateTime}')\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                # print(f'{new_start_dateTime} - {max_date}')\n",
    "                start_dateTime_list.append(new_start_dateTime)\n",
    "                end_dateTime_list.append(max_date) \n",
    "                \n",
    "                new_start_dateTime = max_date+timedelta(seconds=gran_secs.get(gran))\n",
    "                max_date = new_start_dateTime+timedelta(seconds=4000*math.floor(gran_secs.get(gran)))\n",
    "\n",
    "        for i in tqdm.tqdm(range(len(start_dateTime_list)),colour='GREEN', \n",
    "                           ncols=100,desc =f'Downloading {currency_pair} {gran} Data'):\n",
    "\n",
    "            new_df = historical_data(currency_pair=currency_pair, \n",
    "                                    start_date=start_dateTime_list[i],\n",
    "                                    end_date=end_dateTime_list[i],\n",
    "                                    granularity=gran)\n",
    "            \n",
    "            can_df = pd.concat([can_df, new_df])\n",
    "        \n",
    "        return can_df\n",
    "\n",
    "    else:\n",
    "        new_df = historical_data(currency_pair=currency_pair, \n",
    "                                          start_date=start_dateTime_list[i],\n",
    "                                          end_date=end_dateTime_list[i],\n",
    "                                          granularity=gran)\n",
    "\n",
    "\n",
    "master_df = get_all_candles_upgraded(start='08/01/2024',\n",
    "                                     end='08/02/2024',\n",
    "                                     gran='M1',\n",
    "                                     pair='EUR_USD')\n",
    "\n",
    "master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candle_delta(start_date='08/09/2023', end_date='08/09/2024', granularity='M5') -> float:\n",
    "        \"\"\"This will return the number of candles between start_date and end_date.\n",
    "\n",
    "        Args:\n",
    "            start_date: Date to start calculations.\n",
    "            end_date: Date if ending.\n",
    "            granularity: Price update frequency.\n",
    "\n",
    "        Returns:\n",
    "            float: Number of candle sticks between start_date and end_date.\n",
    "        \"\"\"\n",
    "        # Import Necessary Libraries\n",
    "        import pandas as pd\n",
    "\n",
    "        # Convert Dates to dateTimes\n",
    "        start_dateTime = pd.to_datetime(start_date)\n",
    "        end_dateTime = pd.to_datetime(end_date)\n",
    "\n",
    "        # Calculate Seconds Between DateTimes\n",
    "        datetime_dif = end_dateTime - start_dateTime   \n",
    "        seconds_between = datetime_dif.total_seconds() \n",
    "\n",
    "        # Convert second between to number of candles\n",
    "        num_candles = seconds_between / gran_sec_dict.get(granularity)\n",
    "\n",
    "        # Return Number of Candle Sicks\n",
    "        return num_candles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3162240.0\n"
     ]
    }
   ],
   "source": [
    "print(candle_delta(granularity='S10'))\n",
    "gran_secs = {'S5': 5,'S10': 10,'S15': 15,'S30': 30,\n",
    "                'M1': 60,'M2': 120,'M4': 240,'M5': 300,\n",
    "                'M10': 600,'M15': 900,'M30': 1800,\n",
    "                'H1': 3600,'H2': 7200,'H3': 10800,\n",
    "                'H4': 14400,'H6': 21600,'H8': 28800,\n",
    "                'H12': 43200,'D': 86400,'W': 604800,\n",
    "                'M': 2620800}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"datetime.timedelta\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m MAX_CANDLES \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4000\u001b[39m\n\u001b[1;32m      2\u001b[0m start_date \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m08/09/2023\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m max_date \u001b[38;5;241m=\u001b[39m \u001b[43mstart_date\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mtimedelta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_CANDLES\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgran_secs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mM5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(max_date)\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"datetime.timedelta\") to str"
     ]
    }
   ],
   "source": [
    "MAX_CANDLES = 4000\n",
    "start_date = '08/09/2023'\n",
    "\n",
    "max_date = start_date+timedelta(seconds=MAX_CANDLES*math.floor(gran_secs.get('M5')))\n",
    "\n",
    "print(max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCOUNT_ID = \"001-001-2265866-001\"\n",
    "API_TOKEN = \"591691d91a571232b46b9b87e74145ba-4fc5bc3427b4eb7d79c3501316fdffa0\"\n",
    "API_URL = \"api-fxtrade.oanda.com\"\n",
    "\n",
    "from OandaAPI import OandaAPI\n",
    "oa = OandaAPI(account_id=ACCOUNT_ID, api_token=API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historical_data(currency_pair='EUR_USD', start_date='08/09/2023', end_date='08/09/2024', granularity='D', as_pandas=True):\n",
    "        \"\"\"Will get the historical json data for a given currency pair.\n",
    "\n",
    "        Args:\n",
    "            currency_pair: Currency pair name. \n",
    "            start_date: Date of start of historical data.\n",
    "            end_date: Closest date of historical data. \n",
    "            granularity: How often price data is updated.\n",
    "            as_pandas: Set to True to return as pandas dataframe.\n",
    "\n",
    "        Returns:\n",
    "            json: Historical pricing data in json format.\n",
    "            Pandas: Historical pricing dataframe.\n",
    "        \"\"\"\n",
    "        # Must import Necessary Libraries\n",
    "        import time\n",
    "        import requests\n",
    "        import pandas as pd\n",
    "\n",
    "        # Convert Dates to DateTime objects\n",
    "        start_dateTime = pd.to_datetime(start_date)\n",
    "        end_dateTime = pd.to_datetime(end_date)\n",
    "\n",
    "        # Convert Start Date & End Date to a UNIX timestamp\n",
    "        unix_start = time.mktime(start_dateTime.timetuple())\n",
    "        unix_end = time.mktime(end_dateTime.timetuple())\n",
    "\n",
    "        # Authorization Header\n",
    "        auth_header = {\"Authorization\": \"Bearer \"+API_TOKEN} \n",
    "\n",
    "        # Query Parameters\n",
    "        parameters = {\"from\": str(unix_start), \n",
    "                        \"to\": str(unix_end), \n",
    "                        \"granularity\": granularity}\n",
    "        \n",
    "        # Historical Candle Stick Data URL\n",
    "        candle_url = f\"/v3/accounts/{ACCOUNT_ID}/instruments/{currency_pair}/candles\"\n",
    "\n",
    "        # Request for Candle Stick Data\n",
    "        json_response = requests.get(\"https://\"+API_URL+candle_url, headers=auth_header, params=parameters).json()\n",
    "\n",
    "        # Create Pandas DataFrame\n",
    "        if as_pandas is True:\n",
    "            # Convert Json to Pandas df\n",
    "            candle_df = pd.json_normalize(json_response[\"candles\"])\n",
    "\n",
    "            # Drop complete Column\n",
    "            candle_df.drop('complete', inplace=True, axis=1)\n",
    "\n",
    "            # Rename Columns\n",
    "            candle_df.rename(inplace=True,columns={'time':'DateTime','volume':'Volume','mid.o':'Open',\n",
    "                                                    'mid.h':'High','mid.l':'Low','mid.c':'Close'})\n",
    "\n",
    "            # Convert Datetime= Column to datetime object\n",
    "            candle_df['DateTime'] = pd.to_datetime(candle_df['DateTime'])\n",
    "\n",
    "            # Set DateTime to Index\n",
    "            candle_df.set_index('DateTime',inplace=True)\n",
    "\n",
    "            # Restructure Column Names\n",
    "            candle_df = candle_df[['Open','High','Low','Close','Volume']]\n",
    "            \n",
    "            # Return DataFrame\n",
    "            return candle_df\n",
    "        \n",
    "        # Return as Json \n",
    "        else: \n",
    "            return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'candles'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m this_df \u001b[38;5;241m=\u001b[39m \u001b[43mhistorical_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrency_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEUR_USD\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m08/09/2020\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mend_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m08/01/2020\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgranularity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m this_df\n",
      "Cell \u001b[0;32mIn[16], line 45\u001b[0m, in \u001b[0;36mhistorical_data\u001b[0;34m(currency_pair, start_date, end_date, granularity, as_pandas)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Create Pandas DataFrame\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_pandas \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# Convert Json to Pandas df\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     candle_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mjson_normalize(\u001b[43mjson_response\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcandles\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Drop complete Column\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     candle_df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'candles'"
     ]
    }
   ],
   "source": [
    "this_df = historical_data(currency_pair='EUR_USD', start_date='08/09/2020',\n",
    "                                                  end_date='08/01/2020', granularity='D')\n",
    "\n",
    "this_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'candles'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m practice_candle_df \u001b[38;5;241m=\u001b[39m \u001b[43moa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_candlestick_datafame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrency_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEUR_USD\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m08/09/2020\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mend_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m08/01/2020\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgranularity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m practice_candle_df\n",
      "File \u001b[0;32m~/Development/Algo-Research/OandaAPI.py:155\u001b[0m, in \u001b[0;36mOandaAPI.load_candlestick_datafame\u001b[0;34m(self, currency_pair, start_date, end_date, granularity)\u001b[0m\n\u001b[1;32m    150\u001b[0m end_datetime_list \u001b[38;5;241m=\u001b[39m []                                                                \u001b[38;5;66;03m# End DateTime list\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_candles \u001b[38;5;241m<\u001b[39m MAX_CANDLES:\n\u001b[1;32m    153\u001b[0m \n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# Fetch Historical Price Data\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     candles_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistorical_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrency_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrency_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mend_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgranularity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgranularity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mas_pandas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Return Candle Stick DataFrame\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m candles_df\n",
      "File \u001b[0;32m~/Development/Algo-Research/OandaAPI.py:89\u001b[0m, in \u001b[0;36mOandaAPI.historical_data\u001b[0;34m(self, currency_pair, start_date, end_date, granularity, as_pandas)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Create Pandas DataFrame\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_pandas \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# Convert Json to Pandas df\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     candle_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mjson_normalize(\u001b[43mjson_response\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcandles\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Drop complete Column\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     candle_df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'candles'"
     ]
    }
   ],
   "source": [
    "practice_candle_df = oa.load_candlestick_datafame(currency_pair='EUR_USD', start_date='08/09/2020',\n",
    "                                                  end_date='08/01/2020', granularity='D')\n",
    "\n",
    "practice_candle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kyles-DataScience-Enviorment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
